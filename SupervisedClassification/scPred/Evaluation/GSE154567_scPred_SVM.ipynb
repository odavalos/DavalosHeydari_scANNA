{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "54959847-cd76-4593-b93f-c3602bec63c3",
   "metadata": {},
   "source": [
    "# scRNAseq scPred SVMRadial Model Evaluation - GSE154567"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5998c094-37f3-4041-a30f-ee7df526771d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import scanpy as sc\n",
    "import seaborn as sns\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import classification_report as class_rep\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f38cd49a-549f-4b96-9f1b-3d6edcc978ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep\n",
    "dataset_name = 'GSE154567' # label for the dataset\n",
    "dictionary_dir = 'scPred_SVM_SplitDicts' # dir where we save the split dictionaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6e3981b7-a4db-4088-b8df-a52aa111d74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Directory already exists!\n"
     ]
    }
   ],
   "source": [
    "# Make sure necessary directories are avalible\n",
    "\n",
    "# dictionary dir\n",
    "if not os.path.exists(dictionary_dir):\n",
    "    os.makedirs(dictionary_dir)\n",
    "    print(f\"Directory {dictionary_dir} created for saving split dictionaries\")\n",
    "    \n",
    "else:\n",
    "    print('Directory already exists!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4b55036-7559-4fc0-af80-b9e5b127414f",
   "metadata": {},
   "outputs": [],
   "source": [
    "metadata = pd.read_csv('~/NACT_scPred/GSE154567/results/GSE154567_metadata_model_svmRadial.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2753eb2b-d8f6-4e68-9f4a-68a037fdc34d",
   "metadata": {},
   "outputs": [],
   "source": [
    "splits = [f'Split_{i}' for i in range(1,6)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "45cb59a1-31ff-4d2e-aaee-0cdc0171c348",
   "metadata": {},
   "outputs": [],
   "source": [
    "def runSplitEval(metadata, split, cluster_col='celltypes'):\n",
    "    \"\"\"\n",
    "    Running Model Evaluation on multiple splits of the dataset\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Running a Model Evaluation on split: {split}\\n\")\n",
    "    \n",
    "    metadata = metadata[metadata.data_split == split]\n",
    "    \n",
    "    y_test = metadata[cluster_col].tolist()\n",
    "    y_pred = metadata['scpred_prediction'].tolist()\n",
    "    \n",
    "    # model evaluation\n",
    "    acc = accuracy_score(y_test, y_pred)\n",
    "    print(f'Accuracy: {acc:4.4f}')\n",
    "    \n",
    "    # calculating the precision/recall based multi-label F1 score\n",
    "    macro_score = f1_score(y_test, y_pred, average = 'macro' )\n",
    "    w_score = f1_score(y_test, y_pred,average = 'weighted' )\n",
    "    print(f'    -> Non-Weighted F1 Score on validation set: {macro_score:4.4f} ' )\n",
    "    print(f'    -> Weighted F1 Score on validation set: {w_score:4.4f} ' )\n",
    "    print(class_rep(y_test,y_pred))\n",
    "    \n",
    "    return acc, macro_score, w_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8a453891-0398-4f3d-bb23-2f4f0187795a",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_eval = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2d4736c2-1a8b-4967-a8a8-255a2427e784",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working on Split: Split_1\n",
      "Running a Model Evaluation on split: Split_1\n",
      "\n",
      "Accuracy: 0.5593\n",
      "    -> Non-Weighted F1 Score on validation set: 0.6403 \n",
      "    -> Weighted F1 Score on validation set: 0.6906 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                  B cells       0.99      0.63      0.77      1937\n",
      "              CD4 T cells       0.99      0.40      0.57      5064\n",
      "              CD8 T cells       0.91      0.56      0.70      1852\n",
      "                 NK cells       0.95      0.49      0.65       836\n",
      "             erythrocytes       0.96      0.86      0.91      1044\n",
      "         monocytes + cDCs       0.95      0.92      0.94      1212\n",
      "             plasma cells       0.95      0.66      0.78        61\n",
      "proliferating lymphocytes       0.68      0.33      0.44        80\n",
      "               unassigned       0.00      0.00      0.00         0\n",
      " unidentified lymphocytes       0.88      0.53      0.66       888\n",
      "\n",
      "                 accuracy                           0.56     12974\n",
      "                macro avg       0.83      0.54      0.64     12974\n",
      "             weighted avg       0.96      0.56      0.69     12974\n",
      "\n",
      "Working on Split: Split_2\n",
      "Running a Model Evaluation on split: Split_2\n",
      "\n",
      "Accuracy: 0.5760\n",
      "    -> Non-Weighted F1 Score on validation set: 0.6370 \n",
      "    -> Weighted F1 Score on validation set: 0.7044 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                  B cells       0.98      0.62      0.76      1909\n",
      "              CD4 T cells       0.98      0.43      0.60      5012\n",
      "              CD8 T cells       0.91      0.55      0.69      1908\n",
      "                 NK cells       0.93      0.54      0.68       827\n",
      "             erythrocytes       0.96      0.87      0.91      1045\n",
      "         monocytes + cDCs       0.95      0.95      0.95      1217\n",
      "             plasma cells       0.97      0.56      0.71        68\n",
      "proliferating lymphocytes       0.71      0.28      0.40        86\n",
      "               unassigned       0.00      0.00      0.00         0\n",
      " unidentified lymphocytes       0.90      0.53      0.67       902\n",
      "\n",
      "                 accuracy                           0.58     12974\n",
      "                macro avg       0.83      0.53      0.64     12974\n",
      "             weighted avg       0.95      0.58      0.70     12974\n",
      "\n",
      "Working on Split: Split_3\n",
      "Running a Model Evaluation on split: Split_3\n",
      "\n",
      "Accuracy: 0.5684\n",
      "    -> Non-Weighted F1 Score on validation set: 0.6235 \n",
      "    -> Weighted F1 Score on validation set: 0.6992 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                  B cells       0.99      0.62      0.76      1959\n",
      "              CD4 T cells       0.98      0.42      0.59      4937\n",
      "              CD8 T cells       0.93      0.55      0.69      1956\n",
      "                 NK cells       0.96      0.53      0.68       775\n",
      "             erythrocytes       0.96      0.87      0.91      1053\n",
      "         monocytes + cDCs       0.96      0.93      0.94      1266\n",
      "             plasma cells       1.00      0.53      0.69        72\n",
      "proliferating lymphocytes       0.47      0.26      0.33        90\n",
      "               unassigned       0.00      0.00      0.00         0\n",
      " unidentified lymphocytes       0.87      0.49      0.63       866\n",
      "\n",
      "                 accuracy                           0.57     12974\n",
      "                macro avg       0.81      0.52      0.62     12974\n",
      "             weighted avg       0.96      0.57      0.70     12974\n",
      "\n",
      "Working on Split: Split_4\n",
      "Running a Model Evaluation on split: Split_4\n",
      "\n",
      "Accuracy: 0.5735\n",
      "    -> Non-Weighted F1 Score on validation set: 0.6286 \n",
      "    -> Weighted F1 Score on validation set: 0.7022 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                  B cells       0.99      0.62      0.76      1885\n",
      "              CD4 T cells       0.99      0.42      0.58      5065\n",
      "              CD8 T cells       0.92      0.60      0.72      1882\n",
      "                 NK cells       0.95      0.49      0.65       881\n",
      "             erythrocytes       0.96      0.89      0.92      1085\n",
      "         monocytes + cDCs       0.96      0.95      0.96      1213\n",
      "             plasma cells       1.00      0.53      0.70        58\n",
      "proliferating lymphocytes       0.57      0.22      0.32        76\n",
      "               unassigned       0.00      0.00      0.00         0\n",
      " unidentified lymphocytes       0.88      0.54      0.67       829\n",
      "\n",
      "                 accuracy                           0.57     12974\n",
      "                macro avg       0.82      0.53      0.63     12974\n",
      "             weighted avg       0.96      0.57      0.70     12974\n",
      "\n",
      "Working on Split: Split_5\n",
      "Running a Model Evaluation on split: Split_5\n",
      "\n",
      "Accuracy: 0.5765\n",
      "    -> Non-Weighted F1 Score on validation set: 0.6550 \n",
      "    -> Weighted F1 Score on validation set: 0.7044 \n",
      "                           precision    recall  f1-score   support\n",
      "\n",
      "                  B cells       0.99      0.64      0.77      1882\n",
      "              CD4 T cells       0.98      0.40      0.57      5073\n",
      "              CD8 T cells       0.93      0.61      0.73      1834\n",
      "                 NK cells       0.95      0.53      0.68       818\n",
      "             erythrocytes       0.96      0.88      0.92      1092\n",
      "         monocytes + cDCs       0.95      0.93      0.94      1237\n",
      "             plasma cells       0.98      0.71      0.83        63\n",
      "proliferating lymphocytes       0.62      0.31      0.42        64\n",
      "               unassigned       0.00      0.00      0.00         0\n",
      " unidentified lymphocytes       0.89      0.56      0.69       911\n",
      "\n",
      "                 accuracy                           0.58     12974\n",
      "                macro avg       0.83      0.56      0.66     12974\n",
      "             weighted avg       0.96      0.58      0.70     12974\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n",
      "/Users/oscardavalos/miniforge3/envs/scrnaseq_pytorch/lib/python3.9/site-packages/sklearn/metrics/_classification.py:1334: UndefinedMetricWarning: Recall and F-score are ill-defined and being set to 0.0 in labels with no true samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    }
   ],
   "source": [
    "for i in splits:\n",
    "    \n",
    "    print(f\"Working on Split: {i}\")\n",
    "    \n",
    "    # create a dict for storing current split information\n",
    "    split_dict = {'Accuracy':[], \n",
    "                  'Macro_Score':[], \n",
    "                  'Weighted_Score':[]}\n",
    "    acc, macro_score, w_score = runSplitEval(metadata, i, cluster_col='celltypes')\n",
    "    \n",
    "    # joblib.dump(clf, f\"./{model_dir}/{dataset_name}_{i}_RF.pkl\")\n",
    "    \n",
    "    # split_dict['Runtime'].append(total_runtime)\n",
    "    split_dict['Accuracy'].append(acc)\n",
    "    split_dict['Macro_Score'].append(macro_score)\n",
    "    split_dict['Weighted_Score'].append(w_score)\n",
    "    \n",
    "    model_eval[i]=split_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fc69ac0b-4a20-4225-9ecd-d4663eb0a6e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Split_1': {'Accuracy': [0.5593494681671034],\n",
       "  'Macro_Score': [0.6403114466543753],\n",
       "  'Weighted_Score': [0.6906346729578255]},\n",
       " 'Split_2': {'Accuracy': [0.5759981501464467],\n",
       "  'Macro_Score': [0.6370235473506008],\n",
       "  'Weighted_Score': [0.7044245044794037]},\n",
       " 'Split_3': {'Accuracy': [0.5683675042392478],\n",
       "  'Macro_Score': [0.6234864701043467],\n",
       "  'Weighted_Score': [0.6991582213755901]},\n",
       " 'Split_4': {'Accuracy': [0.5735316787420995],\n",
       "  'Macro_Score': [0.6286359547555983],\n",
       "  'Weighted_Score': [0.7022452454881519]},\n",
       " 'Split_5': {'Accuracy': [0.5765376907661477],\n",
       "  'Macro_Score': [0.655002230670555],\n",
       "  'Weighted_Score': [0.7044437677021631]}}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e2a20b22-64cf-4925-b873-9829f9533d38",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the dictionary\n",
    "def Pickler(data, filename):\n",
    "    \n",
    "    outfile = open(filename, 'wb+')\n",
    "    \n",
    "    #source destination\n",
    "    \n",
    "    pickle.dump(data, outfile)\n",
    "    \n",
    "    outfile.close()\n",
    "\n",
    "Pickler(model_eval, filename=f\"{dictionary_dir}/{dataset_name}_svmRadial_EvalDict.pickle\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f52f61-48c0-477c-8697-9d7e3321758e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "scrnaseq_torch",
   "language": "python",
   "name": "scrnaseq_torch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
